{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c2c720-22b0-4bd7-acab-e636b65ff0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting clickhouse_sqlalchemy\n",
      "  Downloading clickhouse-sqlalchemy-0.2.1.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: clickhouse-driver>=0.1.2 in ./.local/lib/python3.10/site-packages (from clickhouse_sqlalchemy) (0.2.4)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from clickhouse_sqlalchemy) (2.25.1)\n",
      "Requirement already satisfied: sqlalchemy<1.5,>=1.4 in ./.local/lib/python3.10/site-packages (from clickhouse_sqlalchemy) (1.4.39)\n",
      "Requirement already satisfied: tzlocal in ./.local/lib/python3.10/site-packages (from clickhouse-driver>=0.1.2->clickhouse_sqlalchemy) (4.2)\n",
      "Requirement already satisfied: pytz in /usr/lib/python3/dist-packages (from clickhouse-driver>=0.1.2->clickhouse_sqlalchemy) (2022.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.local/lib/python3.10/site-packages (from sqlalchemy<1.5,>=1.4->clickhouse_sqlalchemy) (1.1.2)\n",
      "Requirement already satisfied: pytz-deprecation-shim in ./.local/lib/python3.10/site-packages (from tzlocal->clickhouse-driver>=0.1.2->clickhouse_sqlalchemy) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in ./.local/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal->clickhouse-driver>=0.1.2->clickhouse_sqlalchemy) (2022.1)\n",
      "Building wheels for collected packages: clickhouse_sqlalchemy\n",
      "  Building wheel for clickhouse_sqlalchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clickhouse_sqlalchemy: filename=clickhouse_sqlalchemy-0.2.1-py3-none-any.whl size=46617 sha256=55d75755cf83a2ea205318ce5296074f02fc1891666f37c6c0943ba42b6bde23\n",
      "  Stored in directory: /home/sunnykichloo/.cache/pip/wheels/62/5b/6e/cff0644b71936c2411ab91f31d316d5851e26870337e2757d1\n",
      "Successfully built clickhouse_sqlalchemy\n",
      "Installing collected packages: clickhouse_sqlalchemy\n",
      "Successfully installed clickhouse_sqlalchemy-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install clickhouse_sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db601e22-7358-44f9-9b6c-698ac5d50382",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = [ { \"name\": \"Loan Status\", \"type\": \"integer\" }, { \"name\": \"Current Loan Amount\", \"type\": \"integer\" }, { \"name\": \"Term\", \"type\": \"integer\" }, { \"name\": \"Credit Score\", \"type\": \"integer\" }, { \"name\": \"Years in current job\", \"type\": \"integer\" }, { \"name\": \"Home Ownership\", \"type\": \"integer\" }, { \"name\": \"Annual Income\", \"type\": \"decimal\" }, { \"name\": \"Monthly Debt\", \"type\": \"decimal\" }, { \"name\": \"Years of Credit History\", \"type\": \"decimal\" }, { \"name\": \"Months since last delinquent\", \"type\": \"integer\" }, { \"name\": \"Number of Open Accounts\", \"type\": \"integer\" }, { \"name\": \"Number of Credit Problems\", \"type\": \"integer\" }, { \"name\": \"Current Credit Balance\", \"type\": \"integer\" }, { \"name\": \"Maximum Open Credit\", \"type\": \"integer\" }, { \"name\": \"Bankruptcies\", \"type\": \"decimal\" } ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1754101-6af6-4218-9e4b-2776e3b66475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.sql import func as fn, operators\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "from sqlalchemy.sql.expression import asc\n",
    "from sqlalchemy import desc, asc\n",
    "from sqlalchemy import create_engine, select, MetaData, and_, or_, not_\n",
    "from clickhouse_driver import Client\n",
    "from clickhouse_sqlalchemy import (\n",
    "    Table, make_session, get_declarative_base, types, engines\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "class ClickhouseManager:\n",
    "    def __init__(self):\n",
    "        self.client = self.create_client()\n",
    "        self.url = 'clickhouse://{username}:{password}@{host}:{port}/{database}'.format(\n",
    "            username=os.environ['CH_USER'],\n",
    "            password=os.environ['CH_PASSWORD'],\n",
    "            host=os.environ['CH_HOST'],\n",
    "            port=\"8123\",\n",
    "            database=os.environ['CH_DB']\n",
    "        )\n",
    "        print(self.url,flush=True)\n",
    "        self.engine = create_engine(self.url, pool_recycle=3600)\n",
    "        self.conn = self.engine.connect()\n",
    "        self.session = make_session(self.engine)\n",
    "        self.metadata = MetaData(bind=self.engine)\n",
    "        self.Base = get_declarative_base(metadata=self.metadata)\n",
    "\n",
    "    def create_client(self):\n",
    "        client = Client(host=os.environ['CH_HOST'],\n",
    "                        user=os.environ['CH_USER'],\n",
    "                        password=os.environ['CH_PASSWORD'],\n",
    "                        database=os.environ['CH_DB'],settings={'use_numpy': False})\n",
    "        return client\n",
    "\n",
    "    def insert_dataframe(self, query, df):\n",
    "        response = self.client.insert_dataframe(query, df)\n",
    "        return response\n",
    "\n",
    "    \n",
    "    def insert_dataframe_dict(self, query, df):\n",
    "        response = self.client.execute(query, df)\n",
    "        return response\n",
    "    \n",
    "    def execute(self, query):\n",
    "        return self.client.execute(query)\n",
    "\n",
    "    def jsonify(self, query):\n",
    "        query = str(query).replace(\"\\n\", \"\")\n",
    "\n",
    "        print('JSONify: %s' % query)\n",
    "\n",
    "        return f'''select toJSONString(groupArray(t)) from ({query})t;'''\n",
    "\n",
    "    def return_engine(self):\n",
    "        return self.client\n",
    "\n",
    "    def get_query_count(self, query):\n",
    "        count = self.session.execute(query.with_only_columns(\n",
    "            [fn.count()]).order_by(None)).scalar()\n",
    "        return count\n",
    "\n",
    "    def types_mapper(self, type):\n",
    "        # TODO: use for type conversion\n",
    "        print (type)\n",
    "        types_map = {\n",
    "            'integer': \"Nullable(Int32) DEFAULT NULL\",\n",
    "            'float': \"Nullable(Float64) DEFAULT NULL\",\n",
    "            'float32': \"Nullable(Float32) DEFAULT NULL\",\n",
    "            'numeric': \"Nullable(Float64) DEFAULT NULL\",\n",
    "            'string': \"Nullable(String) DEFAULT NULL\",\n",
    "            'text': \"Nullable(String) DEFAULT NULL\",\n",
    "            'enum': \"Enum\",\n",
    "            'array': \"Array\",\n",
    "            'decimal': \"Nullable(Decimal(10, 8))\",\n",
    "            'date': \"timestamp default now()\",\n",
    "            'uuid': \"UUID default generateUUIDv4()\",\n",
    "            'timestamp without time zone': \"timestamp default now()\",\n",
    "            'timestamp with time zone':    \"timestamp default now()\",\n",
    "            'timestamp':                   \"timestamp default now()\",\n",
    "            \n",
    "            'datetime': \"timestamp default now()\"\n",
    "        }\n",
    "        return types_map[type]\n",
    "    \n",
    "\n",
    "    def _column_mapper(self, columns, aggregate):\n",
    "        # TODO: handle timestamp case\n",
    "        columns_map = []\n",
    "        for i in columns:\n",
    "            if i.startswith(\"timestamp:\"):\n",
    "                interval = i.split(':')[1]\n",
    "                columns.append(fn.date_trunc(\n",
    "                    interval, self.table.columns['timestamp']))\n",
    "            else:\n",
    "                columns_map.append(self.table.columns[i])\n",
    "\n",
    "        mapper = {\n",
    "            \"AVERAGE\": fn.avg,\n",
    "            \"MIN\": fn.min,\n",
    "            \"MAX\": fn.max,\n",
    "            \"COUNT\": fn.count,\n",
    "            \"DAY\": fn.date_trunc(\"day\", self.table.columns['timestamp']),\n",
    "            \"MONTH\": fn.date_trunc(\"month\", self.table.columns['timestamp']),\n",
    "            \"HOUR\": fn.date_trunc(\"hour\", self.table.columns['timestamp'])\n",
    "        }\n",
    "        for i in aggregate:\n",
    "            if i['function'] in [\"DAY\", \"MONTH\", \"HOUR\"]:\n",
    "                columns_map.append(mapper[i['function']])\n",
    "            else:\n",
    "                columns_map.append(mapper[i['function']](\n",
    "                    self.table.columns[i['column']]))\n",
    "        if len(columns_map) != 0:\n",
    "            query = select(columns_map)\n",
    "        else:\n",
    "            query = select(self.table.columns)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def __value_transform(self, incoming_array):\n",
    "        # NanArrays=['NaN','nan','Nan','naN']\n",
    "        outgoing_array = []\n",
    "        for i in incoming_array:\n",
    "            if i == 'NaN':\n",
    "                outgoing_array.append(None)\n",
    "            else:\n",
    "                outgoing_array.append(i)\n",
    "        return outgoing_array\n",
    "\n",
    "    def _condition_mapper(self, query, condition, time_range):\n",
    "        and_exp = []\n",
    "        if time_range:\n",
    "            and_exp.append(self._operator_map(\"between\")(\n",
    "                self.table.columns[\"timestamp\"], time_range[0], time_range[1]))\n",
    "        for i in condition[\"and\"]:\n",
    "            if i[\"operator\"] == \"between\":\n",
    "                and_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], i[\"value\"][0], i[\"value\"][1]))\n",
    "            else:\n",
    "                transformed_array = self.__value_transform(i[\"value\"])\n",
    "                and_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], transformed_array))\n",
    "\n",
    "        or_exp = []\n",
    "        for i in condition[\"or\"]:\n",
    "            if i[\"operator\"] == \"between\":\n",
    "                or_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], i[\"value\"][0], i[\"value\"][1]))\n",
    "            else:\n",
    "                transformed_array = self.__value_transform(i[\"value\"])\n",
    "                or_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], transformed_array))\n",
    "\n",
    "        not_exp = []\n",
    "        for i in condition[\"not\"]:\n",
    "            if i[\"operator\"] == \"between\":\n",
    "                not_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], i[\"value\"][0], i[\"value\"][1]))\n",
    "            else:\n",
    "                transformed_array = self.__value_transform(i[\"value\"])\n",
    "                not_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], transformed_array))\n",
    "\n",
    "        if len(and_exp) != 0:\n",
    "            query = query.where(and_(*and_exp))\n",
    "        if len(or_exp) != 0:\n",
    "            query = query.where(or_(*or_exp))\n",
    "        if len(not_exp) != 0:\n",
    "            query = query.where(not_(*not_exp))\n",
    "        return query\n",
    "\n",
    "    def _operator_map(self, key):\n",
    "        operators_map = {\n",
    "            \"<=\": operators.ColumnOperators.__le__,\n",
    "            \"<\": operators.ColumnOperators.__lt__,\n",
    "            \">=\": operators.ColumnOperators.__ge__,\n",
    "            \">\": operators.ColumnOperators.__gt__,\n",
    "            \"!=\": operators.ColumnOperators.__ne__,\n",
    "            \"=\": operators.ColumnOperators.__eq__,\n",
    "            \"between\": operators.ColumnOperators.between,\n",
    "            \"in\": operators.ColumnOperators.in_,\n",
    "            \"nin\": operators.ColumnOperators.notin_,\n",
    "            \"is_not\": operators.ColumnOperators.isnot\n",
    "        }\n",
    "        return operators_map.get(key)\n",
    "\n",
    "    def _order_by_mapper(self, query, order_list):\n",
    "        ord_map = {'ASC': asc, 'DESC': desc}\n",
    "        order_map_list = []\n",
    "        for i in order_list:\n",
    "            order_map_list.append(ord_map[i[\"order\"]](\n",
    "                self.table.columns[i[\"column\"]]))\n",
    "        if len(order_map_list) != 0:\n",
    "            for order in order_map_list:\n",
    "                query = query.order_by(order)\n",
    "        return query\n",
    "\n",
    "    def _limit_mapper(self, query, limit):\n",
    "        return query.limit(limit)\n",
    "\n",
    "    def _offset_mapper(self, query, offset):\n",
    "        return query.offset(offset)\n",
    "\n",
    "    def _group_by_mapper(self, query, group_list):\n",
    "        grp_list = []\n",
    "        for i in group_list:\n",
    "            if i.startswith('timestamp:'):\n",
    "                interval = i.split(':')[1]\n",
    "                grp_list.append(fn.date_trunc(\n",
    "                    interval, self.table.columns['timestamp']))\n",
    "            else:\n",
    "                grp_list.append(self.table.columns[i])\n",
    "\n",
    "        if len(grp_list) != 0:\n",
    "            query = query.group_by(*grp_list)\n",
    "        return query\n",
    "\n",
    "    def json_to_sql(self, json_query, table_name, time_range, raw=False):\n",
    "\n",
    "        print('Converting to JSON: ', json_query)\n",
    "\n",
    "        self.table = Table(table_name, self.metadata,\n",
    "                           autoload=True, autoload_with=self.engine)\n",
    "\n",
    "        query = self._column_mapper(\n",
    "            json_query['columns'], json_query['aggregate'])\n",
    "\n",
    "        query = self._condition_mapper(\n",
    "            query, json_query[\"conditions\"], time_range)\n",
    "\n",
    "        query = self._group_by_mapper(query, json_query[\"group_by\"])\n",
    "\n",
    "        query = self._order_by_mapper(query, json_query[\"order_by\"])\n",
    "\n",
    "        if json_query.get('limit'):\n",
    "            query = self._limit_mapper(query, json_query[\"limit\"])\n",
    "\n",
    "        if json_query.get('offset'):\n",
    "            query = self._offset_mapper(query, json_query[\"offset\"])\n",
    "\n",
    "        if not raw:\n",
    "            query = query.compile(compile_kwargs={\"literal_binds\": True})\n",
    "\n",
    "        return query\n",
    "\n",
    "    def sql_to_json(self):\n",
    "        pass\n",
    "\n",
    "    def merge_json(self, query_1, query_2):\n",
    "\n",
    "        print('Merging Queries: ', query_1, query_2)\n",
    "\n",
    "        for col in query_2.get('columns'):\n",
    "            query_1.get('columns').append(col)\n",
    "\n",
    "        for agg in query_2.get('aggregate'):\n",
    "            query_1.get('aggregate').append(agg)\n",
    "\n",
    "        for grp in query_2.get('group_by'):\n",
    "            query_1.get('group_by').append(grp)\n",
    "\n",
    "        for ord in query_2.get('order_by'):\n",
    "            query_1.get('order_by').append(ord)\n",
    "\n",
    "        for and_cond in query_2.get('conditions').get('and'):\n",
    "            query_1.get('conditions').get('and').append(and_cond)\n",
    "\n",
    "        for or_cond in query_2.get('conditions').get('or'):\n",
    "            query_1.get('conditions').get('or').append(or_cond)\n",
    "\n",
    "        for not_cond in query_2.get('conditions').get('not'):\n",
    "            query_1.get('conditions').get('not').append(not_cond)\n",
    "\n",
    "        if query_2.get('limit'):\n",
    "            query_1['limit'] = query_2.get('limit')\n",
    "\n",
    "        if query_2.get('offset'):\n",
    "            query_1['offset'] = query_2.get('offset')\n",
    "\n",
    "        return query_1\n",
    "\n",
    "    def query(self, query_expression):\n",
    "        res = self.session.execute(query_expression)\n",
    "        return res.fetchall()\n",
    "\n",
    "\n",
    "clickhouse_manager = None\n",
    "\n",
    "\n",
    "def initialize():\n",
    "    global clickhouse_manager\n",
    "    clickhouse_manager = ClickhouseManager()\n",
    "\n",
    "\n",
    "def get_instance():\n",
    "    global clickhouse_manager\n",
    "    if not clickhouse_manager:\n",
    "        initialize()\n",
    "    return clickhouse_manager\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b102a4c4-c92f-4fed-960c-c1819b7112c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.sql import func as fn, operators\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "from sqlalchemy.sql.expression import asc\n",
    "from sqlalchemy import desc, asc\n",
    "from sqlalchemy import create_engine, select, MetaData, and_, or_, not_\n",
    "from clickhouse_driver import Client\n",
    "from clickhouse_sqlalchemy import (\n",
    "    Table, make_session, get_declarative_base, types, engines\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "class ClickhouseManager:\n",
    "    def __init__(self):\n",
    "        self.client = self.create_client()\n",
    "        self.url = 'clickhouse://{username}:{password}@{host}:{port}/{database}'.format(\n",
    "            username=os.environ['CH_USER'],\n",
    "            password=os.environ['CH_PASSWORD'],\n",
    "            host=os.environ['CH_HOST'],\n",
    "            port=\"8123\",\n",
    "            database=os.environ['CH_DB']\n",
    "        )\n",
    "        print(self.url,flush=True)\n",
    "        self.engine = create_engine(self.url, pool_recycle=3600)\n",
    "        self.conn = self.engine.connect()\n",
    "        self.session = make_session(self.engine)\n",
    "        self.metadata = MetaData(bind=self.engine)\n",
    "        self.Base = get_declarative_base(metadata=self.metadata)\n",
    "\n",
    "    def create_client(self):\n",
    "        client = Client(host=os.environ['CH_HOST'],\n",
    "                        user=os.environ['CH_USER'],\n",
    "                        password=os.environ['CH_PASSWORD'],\n",
    "                        database=os.environ['CH_DB'],settings={'use_numpy': False})\n",
    "        return client\n",
    "\n",
    "    def insert_dataframe(self, query, df):\n",
    "        response = self.client.insert_dataframe(query, df)\n",
    "        return response\n",
    "\n",
    "    \n",
    "    def insert_dataframe_dict(self, query, df):\n",
    "        response = self.client.execute(query, df)\n",
    "        return response\n",
    "    \n",
    "    def execute(self, query):\n",
    "        return self.client.execute(query)\n",
    "\n",
    "    def jsonify(self, query):\n",
    "        query = str(query).replace(\"\\n\", \"\")\n",
    "\n",
    "        print('JSONify: %s' % query)\n",
    "\n",
    "        return f'''select toJSONString(groupArray(t)) from ({query})t;'''\n",
    "\n",
    "    def return_engine(self):\n",
    "        return self.client\n",
    "\n",
    "    def get_query_count(self, query):\n",
    "        count = self.session.execute(query.with_only_columns(\n",
    "            [fn.count()]).order_by(None)).scalar()\n",
    "        return count\n",
    "\n",
    "    def types_mapper(self, type):\n",
    "        # TODO: use for type conversion\n",
    "        print (type)\n",
    "        types_map = {\n",
    "            'integer': \"Nullable(Int32) DEFAULT NULL\",\n",
    "            'float': \"Nullable(Float64) DEFAULT NULL\",\n",
    "            'float32': \"Nullable(Float32) DEFAULT NULL\",\n",
    "            'numeric': \"Nullable(Float64) DEFAULT NULL\",\n",
    "            'string': \"Nullable(String) DEFAULT NULL\",\n",
    "            'text': \"Nullable(String) DEFAULT NULL\",\n",
    "            'enum': \"Enum\",\n",
    "            'array': \"Array\",\n",
    "            'decimal': \"Nullable(Decimal(10, 8))\",\n",
    "            'date': \"timestamp default now()\",\n",
    "            'uuid': \"UUID default generateUUIDv4()\",\n",
    "            'timestamp without time zone': \"timestamp default now()\",\n",
    "            'timestamp with time zone':    \"timestamp default now()\",\n",
    "            'timestamp':                   \"timestamp default now()\",\n",
    "            \n",
    "            'datetime': \"timestamp default now()\"\n",
    "        }\n",
    "        return types_map[type]\n",
    "    \n",
    "\n",
    "    def _column_mapper(self, columns, aggregate):\n",
    "        # TODO: handle timestamp case\n",
    "        columns_map = []\n",
    "        for i in columns:\n",
    "            if i.startswith(\"timestamp:\"):\n",
    "                interval = i.split(':')[1]\n",
    "                columns.append(fn.date_trunc(\n",
    "                    interval, self.table.columns['timestamp']))\n",
    "            else:\n",
    "                columns_map.append(self.table.columns[i])\n",
    "\n",
    "        mapper = {\n",
    "            \"AVERAGE\": fn.avg,\n",
    "            \"MIN\": fn.min,\n",
    "            \"MAX\": fn.max,\n",
    "            \"COUNT\": fn.count,\n",
    "            \"DAY\": fn.date_trunc(\"day\", self.table.columns['timestamp']),\n",
    "            \"MONTH\": fn.date_trunc(\"month\", self.table.columns['timestamp']),\n",
    "            \"HOUR\": fn.date_trunc(\"hour\", self.table.columns['timestamp'])\n",
    "        }\n",
    "        for i in aggregate:\n",
    "            if i['function'] in [\"DAY\", \"MONTH\", \"HOUR\"]:\n",
    "                columns_map.append(mapper[i['function']])\n",
    "            else:\n",
    "                columns_map.append(mapper[i['function']](\n",
    "                    self.table.columns[i['column']]))\n",
    "        if len(columns_map) != 0:\n",
    "            query = select(columns_map)\n",
    "        else:\n",
    "            query = select(self.table.columns)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def __value_transform(self, incoming_array):\n",
    "        # NanArrays=['NaN','nan','Nan','naN']\n",
    "        outgoing_array = []\n",
    "        for i in incoming_array:\n",
    "            if i == 'NaN':\n",
    "                outgoing_array.append(None)\n",
    "            else:\n",
    "                outgoing_array.append(i)\n",
    "        return outgoing_array\n",
    "\n",
    "    def _condition_mapper(self, query, condition, time_range):\n",
    "        and_exp = []\n",
    "        if time_range:\n",
    "            and_exp.append(self._operator_map(\"between\")(\n",
    "                self.table.columns[\"timestamp\"], time_range[0], time_range[1]))\n",
    "        for i in condition[\"and\"]:\n",
    "            if i[\"operator\"] == \"between\":\n",
    "                and_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], i[\"value\"][0], i[\"value\"][1]))\n",
    "            else:\n",
    "                transformed_array = self.__value_transform(i[\"value\"])\n",
    "                and_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], transformed_array))\n",
    "\n",
    "        or_exp = []\n",
    "        for i in condition[\"or\"]:\n",
    "            if i[\"operator\"] == \"between\":\n",
    "                or_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], i[\"value\"][0], i[\"value\"][1]))\n",
    "            else:\n",
    "                transformed_array = self.__value_transform(i[\"value\"])\n",
    "                or_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], transformed_array))\n",
    "\n",
    "        not_exp = []\n",
    "        for i in condition[\"not\"]:\n",
    "            if i[\"operator\"] == \"between\":\n",
    "                not_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], i[\"value\"][0], i[\"value\"][1]))\n",
    "            else:\n",
    "                transformed_array = self.__value_transform(i[\"value\"])\n",
    "                not_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], transformed_array))\n",
    "\n",
    "        if len(and_exp) != 0:\n",
    "            query = query.where(and_(*and_exp))\n",
    "        if len(or_exp) != 0:\n",
    "            query = query.where(or_(*or_exp))\n",
    "        if len(not_exp) != 0:\n",
    "            query = query.where(not_(*not_exp))\n",
    "        return query\n",
    "\n",
    "    def _operator_map(self, key):\n",
    "        operators_map = {\n",
    "            \"<=\": operators.ColumnOperators.__le__,\n",
    "            \"<\": operators.ColumnOperators.__lt__,\n",
    "            \">=\": operators.ColumnOperators.__ge__,\n",
    "            \">\": operators.ColumnOperators.__gt__,\n",
    "            \"!=\": operators.ColumnOperators.__ne__,\n",
    "            \"=\": operators.ColumnOperators.__eq__,\n",
    "            \"between\": operators.ColumnOperators.between,\n",
    "            \"in\": operators.ColumnOperators.in_,\n",
    "            \"nin\": operators.ColumnOperators.notin_,\n",
    "            \"is_not\": operators.ColumnOperators.isnot\n",
    "        }\n",
    "        return operators_map.get(key)\n",
    "\n",
    "    def _order_by_mapper(self, query, order_list):\n",
    "        ord_map = {'ASC': asc, 'DESC': desc}\n",
    "        order_map_list = []\n",
    "        for i in order_list:\n",
    "            order_map_list.append(ord_map[i[\"order\"]](\n",
    "                self.table.columns[i[\"column\"]]))\n",
    "        if len(order_map_list) != 0:\n",
    "            for order in order_map_list:\n",
    "                query = query.order_by(order)\n",
    "        return query\n",
    "\n",
    "    def _limit_mapper(self, query, limit):\n",
    "        return query.limit(limit)\n",
    "\n",
    "    def _offset_mapper(self, query, offset):\n",
    "        return query.offset(offset)\n",
    "\n",
    "    def _group_by_mapper(self, query, group_list):\n",
    "        grp_list = []\n",
    "        for i in group_list:\n",
    "            if i.startswith('timestamp:'):\n",
    "                interval = i.split(':')[1]\n",
    "                grp_list.append(fn.date_trunc(\n",
    "                    interval, self.table.columns['timestamp']))\n",
    "            else:\n",
    "                grp_list.append(self.table.columns[i])\n",
    "\n",
    "        if len(grp_list) != 0:\n",
    "            query = query.group_by(*grp_list)\n",
    "        return query\n",
    "\n",
    "    def json_to_sql(self, json_query, table_name, time_range, raw=False):\n",
    "\n",
    "        print('Converting to JSON: ', json_query)\n",
    "\n",
    "        self.table = Table(table_name, self.metadata,\n",
    "                           autoload=True, autoload_with=self.engine)\n",
    "\n",
    "        query = self._column_mapper(\n",
    "            json_query['columns'], json_query['aggregate'])\n",
    "\n",
    "        query = self._condition_mapper(\n",
    "            query, json_query[\"conditions\"], time_range)\n",
    "\n",
    "        query = self._group_by_mapper(query, json_query[\"group_by\"])\n",
    "\n",
    "        query = self._order_by_mapper(query, json_query[\"order_by\"])\n",
    "\n",
    "        if json_query.get('limit'):\n",
    "            query = self._limit_mapper(query, json_query[\"limit\"])\n",
    "\n",
    "        if json_query.get('offset'):\n",
    "            query = self._offset_mapper(query, json_query[\"offset\"])\n",
    "\n",
    "        if not raw:\n",
    "            query = query.compile(compile_kwargs={\"literal_binds\": True})\n",
    "\n",
    "        return query\n",
    "\n",
    "    def sql_to_json(self):\n",
    "        pass\n",
    "\n",
    "    def merge_json(self, query_1, query_2):\n",
    "\n",
    "        print('Merging Queries: ', query_1, query_2)\n",
    "\n",
    "        for col in query_2.get('columns'):\n",
    "            query_1.get('columns').append(col)\n",
    "\n",
    "        for agg in query_2.get('aggregate'):\n",
    "            query_1.get('aggregate').append(agg)\n",
    "\n",
    "        for grp in query_2.get('group_by'):\n",
    "            query_1.get('group_by').append(grp)\n",
    "\n",
    "        for ord in query_2.get('order_by'):\n",
    "            query_1.get('order_by').append(ord)\n",
    "\n",
    "        for and_cond in query_2.get('conditions').get('and'):\n",
    "            query_1.get('conditions').get('and').append(and_cond)\n",
    "\n",
    "        for or_cond in query_2.get('conditions').get('or'):\n",
    "            query_1.get('conditions').get('or').append(or_cond)\n",
    "\n",
    "        for not_cond in query_2.get('conditions').get('not'):\n",
    "            query_1.get('conditions').get('not').append(not_cond)\n",
    "\n",
    "        if query_2.get('limit'):\n",
    "            query_1['limit'] = query_2.get('limit')\n",
    "\n",
    "        if query_2.get('offset'):\n",
    "            query_1['offset'] = query_2.get('offset')\n",
    "\n",
    "        return query_1\n",
    "\n",
    "    def query(self, query_expression):\n",
    "        res = self.session.execute(query_expression)\n",
    "        return res.fetchall()\n",
    "\n",
    "\n",
    "clickhouse_manager = None\n",
    "\n",
    "\n",
    "def initialize():\n",
    "    global clickhouse_manager\n",
    "    clickhouse_manager = ClickhouseManager()\n",
    "\n",
    "\n",
    "def get_instance():\n",
    "    global clickhouse_manager\n",
    "    if not clickhouse_manager:\n",
    "        initialize()\n",
    "    return clickhouse_manager\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66cd0b5e-9066-4042-9ed2-5c7a5dbe3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.sql import func as fn, operators\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "from sqlalchemy.sql.expression import asc\n",
    "from sqlalchemy import desc, asc\n",
    "from sqlalchemy import create_engine, select, MetaData, and_, or_, not_\n",
    "from clickhouse_driver import Client\n",
    "from clickhouse_sqlalchemy import (\n",
    "    Table, make_session, get_declarative_base, types, engines\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "class ClickhouseManager:\n",
    "    def __init__(self):\n",
    "        self.client = self.create_client()\n",
    "        self.url = 'clickhouse://{username}:{password}@{host}:{port}/{database}'.format(\n",
    "            username=os.environ['CH_USER'],\n",
    "            password=os.environ['CH_PASSWORD'],\n",
    "            host=os.environ['CH_HOST'],\n",
    "            port=\"8123\",\n",
    "            database=os.environ['CH_DB']\n",
    "        )\n",
    "        print(self.url,flush=True)\n",
    "        self.engine = create_engine(self.url, pool_recycle=3600)\n",
    "        self.conn = self.engine.connect()\n",
    "        self.session = make_session(self.engine)\n",
    "        self.metadata = MetaData(bind=self.engine)\n",
    "        self.Base = get_declarative_base(metadata=self.metadata)\n",
    "\n",
    "    def create_client(self):\n",
    "        client = Client(host=os.environ['CH_HOST'],\n",
    "                        user=os.environ['CH_USER'],\n",
    "                        password=os.environ['CH_PASSWORD'],\n",
    "                        database=os.environ['CH_DB'],settings={'use_numpy': False})\n",
    "        return client\n",
    "\n",
    "    def insert_dataframe(self, query, df):\n",
    "        response = self.client.insert_dataframe(query, df)\n",
    "        return response\n",
    "\n",
    "    \n",
    "    def insert_dataframe_dict(self, query, df):\n",
    "        response = self.client.execute(query, df)\n",
    "        return response\n",
    "    \n",
    "    def execute(self, query):\n",
    "        return self.client.execute(query)\n",
    "\n",
    "    def jsonify(self, query):\n",
    "        query = str(query).replace(\"\\n\", \"\")\n",
    "\n",
    "        print('JSONify: %s' % query)\n",
    "\n",
    "        return f'''select toJSONString(groupArray(t)) from ({query})t;'''\n",
    "\n",
    "    def return_engine(self):\n",
    "        return self.client\n",
    "\n",
    "    def get_query_count(self, query):\n",
    "        count = self.session.execute(query.with_only_columns(\n",
    "            [fn.count()]).order_by(None)).scalar()\n",
    "        return count\n",
    "\n",
    "    def types_mapper(self, type):\n",
    "        # TODO: use for type conversion\n",
    "        print (type)\n",
    "        types_map = {\n",
    "            'integer': \"Nullable(Int32) DEFAULT NULL\",\n",
    "            'float': \"Nullable(Float64) DEFAULT NULL\",\n",
    "            'float32': \"Nullable(Float32) DEFAULT NULL\",\n",
    "            'numeric': \"Nullable(Float64) DEFAULT NULL\",\n",
    "            'string': \"Nullable(String) DEFAULT NULL\",\n",
    "            'text': \"Nullable(String) DEFAULT NULL\",\n",
    "            'enum': \"Enum\",\n",
    "            'array': \"Array\",\n",
    "            'decimal': \"Nullable(Decimal(10, 8))\",\n",
    "            'date': \"timestamp default now()\",\n",
    "            'uuid': \"UUID default generateUUIDv4()\",\n",
    "            'timestamp without time zone': \"timestamp default now()\",\n",
    "            'timestamp with time zone':    \"timestamp default now()\",\n",
    "            'timestamp':                   \"timestamp default now()\",\n",
    "            \n",
    "            'datetime': \"timestamp default now()\"\n",
    "        }\n",
    "        return types_map[type]\n",
    "    \n",
    "\n",
    "    def _column_mapper(self, columns, aggregate):\n",
    "        # TODO: handle timestamp case\n",
    "        columns_map = []\n",
    "        for i in columns:\n",
    "            if i.startswith(\"timestamp:\"):\n",
    "                interval = i.split(':')[1]\n",
    "                columns.append(fn.date_trunc(\n",
    "                    interval, self.table.columns['timestamp']))\n",
    "            else:\n",
    "                columns_map.append(self.table.columns[i])\n",
    "\n",
    "        mapper = {\n",
    "            \"AVERAGE\": fn.avg,\n",
    "            \"MIN\": fn.min,\n",
    "            \"MAX\": fn.max,\n",
    "            \"COUNT\": fn.count,\n",
    "            \"DAY\": fn.date_trunc(\"day\", self.table.columns['timestamp']),\n",
    "            \"MONTH\": fn.date_trunc(\"month\", self.table.columns['timestamp']),\n",
    "            \"HOUR\": fn.date_trunc(\"hour\", self.table.columns['timestamp'])\n",
    "        }\n",
    "        for i in aggregate:\n",
    "            if i['function'] in [\"DAY\", \"MONTH\", \"HOUR\"]:\n",
    "                columns_map.append(mapper[i['function']])\n",
    "            else:\n",
    "                columns_map.append(mapper[i['function']](\n",
    "                    self.table.columns[i['column']]))\n",
    "        if len(columns_map) != 0:\n",
    "            query = select(columns_map)\n",
    "        else:\n",
    "            query = select(self.table.columns)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def __value_transform(self, incoming_array):\n",
    "        # NanArrays=['NaN','nan','Nan','naN']\n",
    "        outgoing_array = []\n",
    "        for i in incoming_array:\n",
    "            if i == 'NaN':\n",
    "                outgoing_array.append(None)\n",
    "            else:\n",
    "                outgoing_array.append(i)\n",
    "        return outgoing_array\n",
    "\n",
    "    def _condition_mapper(self, query, condition, time_range):\n",
    "        and_exp = []\n",
    "        if time_range:\n",
    "            and_exp.append(self._operator_map(\"between\")(\n",
    "                self.table.columns[\"timestamp\"], time_range[0], time_range[1]))\n",
    "        for i in condition[\"and\"]:\n",
    "            if i[\"operator\"] == \"between\":\n",
    "                and_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], i[\"value\"][0], i[\"value\"][1]))\n",
    "            else:\n",
    "                transformed_array = self.__value_transform(i[\"value\"])\n",
    "                and_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], transformed_array))\n",
    "\n",
    "        or_exp = []\n",
    "        for i in condition[\"or\"]:\n",
    "            if i[\"operator\"] == \"between\":\n",
    "                or_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], i[\"value\"][0], i[\"value\"][1]))\n",
    "            else:\n",
    "                transformed_array = self.__value_transform(i[\"value\"])\n",
    "                or_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], transformed_array))\n",
    "\n",
    "        not_exp = []\n",
    "        for i in condition[\"not\"]:\n",
    "            if i[\"operator\"] == \"between\":\n",
    "                not_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], i[\"value\"][0], i[\"value\"][1]))\n",
    "            else:\n",
    "                transformed_array = self.__value_transform(i[\"value\"])\n",
    "                not_exp.append(self._operator_map(i[\"operator\"])(\n",
    "                    self.table.columns[i[\"column\"]], transformed_array))\n",
    "\n",
    "        if len(and_exp) != 0:\n",
    "            query = query.where(and_(*and_exp))\n",
    "        if len(or_exp) != 0:\n",
    "            query = query.where(or_(*or_exp))\n",
    "        if len(not_exp) != 0:\n",
    "            query = query.where(not_(*not_exp))\n",
    "        return query\n",
    "\n",
    "    def _operator_map(self, key):\n",
    "        operators_map = {\n",
    "            \"<=\": operators.ColumnOperators.__le__,\n",
    "            \"<\": operators.ColumnOperators.__lt__,\n",
    "            \">=\": operators.ColumnOperators.__ge__,\n",
    "            \">\": operators.ColumnOperators.__gt__,\n",
    "            \"!=\": operators.ColumnOperators.__ne__,\n",
    "            \"=\": operators.ColumnOperators.__eq__,\n",
    "            \"between\": operators.ColumnOperators.between,\n",
    "            \"in\": operators.ColumnOperators.in_,\n",
    "            \"nin\": operators.ColumnOperators.notin_,\n",
    "            \"is_not\": operators.ColumnOperators.isnot\n",
    "        }\n",
    "        return operators_map.get(key)\n",
    "\n",
    "    def _order_by_mapper(self, query, order_list):\n",
    "        ord_map = {'ASC': asc, 'DESC': desc}\n",
    "        order_map_list = []\n",
    "        for i in order_list:\n",
    "            order_map_list.append(ord_map[i[\"order\"]](\n",
    "                self.table.columns[i[\"column\"]]))\n",
    "        if len(order_map_list) != 0:\n",
    "            for order in order_map_list:\n",
    "                query = query.order_by(order)\n",
    "        return query\n",
    "\n",
    "    def _limit_mapper(self, query, limit):\n",
    "        return query.limit(limit)\n",
    "\n",
    "    def _offset_mapper(self, query, offset):\n",
    "        return query.offset(offset)\n",
    "\n",
    "    def _group_by_mapper(self, query, group_list):\n",
    "        grp_list = []\n",
    "        for i in group_list:\n",
    "            if i.startswith('timestamp:'):\n",
    "                interval = i.split(':')[1]\n",
    "                grp_list.append(fn.date_trunc(\n",
    "                    interval, self.table.columns['timestamp']))\n",
    "            else:\n",
    "                grp_list.append(self.table.columns[i])\n",
    "\n",
    "        if len(grp_list) != 0:\n",
    "            query = query.group_by(*grp_list)\n",
    "        return query\n",
    "\n",
    "    def json_to_sql(self, json_query, table_name, time_range, raw=False):\n",
    "\n",
    "        print('Converting to JSON: ', json_query)\n",
    "\n",
    "        self.table = Table(table_name, self.metadata,\n",
    "                           autoload=True, autoload_with=self.engine)\n",
    "\n",
    "        query = self._column_mapper(\n",
    "            json_query['columns'], json_query['aggregate'])\n",
    "\n",
    "        query = self._condition_mapper(\n",
    "            query, json_query[\"conditions\"], time_range)\n",
    "\n",
    "        query = self._group_by_mapper(query, json_query[\"group_by\"])\n",
    "\n",
    "        query = self._order_by_mapper(query, json_query[\"order_by\"])\n",
    "\n",
    "        if json_query.get('limit'):\n",
    "            query = self._limit_mapper(query, json_query[\"limit\"])\n",
    "\n",
    "        if json_query.get('offset'):\n",
    "            query = self._offset_mapper(query, json_query[\"offset\"])\n",
    "\n",
    "        if not raw:\n",
    "            query = query.compile(compile_kwargs={\"literal_binds\": True})\n",
    "\n",
    "        return query\n",
    "\n",
    "    def sql_to_json(self):\n",
    "        pass\n",
    "\n",
    "    def merge_json(self, query_1, query_2):\n",
    "\n",
    "        print('Merging Queries: ', query_1, query_2)\n",
    "\n",
    "        for col in query_2.get('columns'):\n",
    "            query_1.get('columns').append(col)\n",
    "\n",
    "        for agg in query_2.get('aggregate'):\n",
    "            query_1.get('aggregate').append(agg)\n",
    "\n",
    "        for grp in query_2.get('group_by'):\n",
    "            query_1.get('group_by').append(grp)\n",
    "\n",
    "        for ord in query_2.get('order_by'):\n",
    "            query_1.get('order_by').append(ord)\n",
    "\n",
    "        for and_cond in query_2.get('conditions').get('and'):\n",
    "            query_1.get('conditions').get('and').append(and_cond)\n",
    "\n",
    "        for or_cond in query_2.get('conditions').get('or'):\n",
    "            query_1.get('conditions').get('or').append(or_cond)\n",
    "\n",
    "        for not_cond in query_2.get('conditions').get('not'):\n",
    "            query_1.get('conditions').get('not').append(not_cond)\n",
    "\n",
    "        if query_2.get('limit'):\n",
    "            query_1['limit'] = query_2.get('limit')\n",
    "\n",
    "        if query_2.get('offset'):\n",
    "            query_1['offset'] = query_2.get('offset')\n",
    "\n",
    "        return query_1\n",
    "\n",
    "    def query(self, query_expression):\n",
    "        res = self.session.execute(query_expression)\n",
    "        return res.fetchall()\n",
    "\n",
    "\n",
    "clickhouse_manager = None\n",
    "\n",
    "\n",
    "def initialize():\n",
    "    global clickhouse_manager\n",
    "    clickhouse_manager = ClickhouseManager()\n",
    "\n",
    "\n",
    "def get_instance():\n",
    "    global clickhouse_manager\n",
    "    if not clickhouse_manager:\n",
    "        initialize()\n",
    "    return clickhouse_manager\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a6889f3-1201-4d87-b527-4f9774c25c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_231657/2773762229.py:10: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('dataset_loan_profile_1183.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feat_loan_status': Int64Dtype(), 'feat_current_loan_amount': Int64Dtype(), 'feat_term': Int64Dtype(), 'feat_credit_score': Int64Dtype(), 'feat_years_in_current_job': Int64Dtype(), 'feat_home_ownership': Int64Dtype(), 'feat_annual_income': Float64Dtype(), 'feat_monthly_debt': Float64Dtype(), 'feat_years_of_credit_history': Float64Dtype(), 'feat_months_since_last_delinquent': Int64Dtype(), 'feat_number_of_open_accounts': Int64Dtype(), 'feat_number_of_credit_problems': Int64Dtype(), 'feat_current_credit_balance': Int64Dtype(), 'feat_maximum_open_credit': Int64Dtype(), 'feat_bankruptcies': Float64Dtype(), 'id': string[python], 'timestamp': dtype('<M8[ns]'), 'model_id': dtype('O'), 'event_type': string[python], 'log_id': string[python], 'created_at': string[python], 'updated_at': string[python], 'prediction_score_loan_status': dtype('O'), 'prediction_class_loan_status': Int64Dtype(), 'actual_loan_status': Int64Dtype()}\n"
     ]
    }
   ],
   "source": [
    "ch_driver_obj = get_instance()\n",
    "\n",
    "#response = ch_driver_obj.execute(query)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import decimal\n",
    "import io\n",
    "import csv\n",
    "df = pd.read_csv('dataset_loan_profile_1183.csv')\n",
    "#print(df)\n",
    "df = df.rename(columns=lambda s:s.lower().replace(' ','_'))\n",
    "#print(list(df.columns.values)) \n",
    "cls=\", \".join(list(df.columns.values))\n",
    "df = df.astype({\"timestamp\": np.datetime64})\n",
    "df= df.replace({np.nan: None})\n",
    "#print(df.dtypes)\n",
    "df = df.convert_dtypes()\n",
    "headers=df.dtypes.to_dict()\n",
    "array_from_df = df.to_records()\n",
    "df[\"timestamp\"] = df[\"timestamp\"].map(lambda col:np.datetime64(col))\n",
    "df[\"updated_at\"]=df[\"timestamp\"]\n",
    "df= df.replace({np.nan: None})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#headers=df.dtypes.to_dict()\n",
    "\n",
    "#headers = df.columns.str.lower()\n",
    "\n",
    "print(headers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ca84d43-db77-42a9-836a-b2f68cfe4f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'column_name': 'feat_loan_status', 'data_type': 'integer'}, {'column_name': 'feat_current_loan_amount', 'data_type': 'integer'}, {'column_name': 'feat_term', 'data_type': 'integer'}, {'column_name': 'feat_credit_score', 'data_type': 'integer'}, {'column_name': 'feat_years_in_current_job', 'data_type': 'integer'}, {'column_name': 'feat_home_ownership', 'data_type': 'integer'}, {'column_name': 'feat_annual_income', 'data_type': 'numeric'}, {'column_name': 'feat_monthly_debt', 'data_type': 'numeric'}, {'column_name': 'feat_years_of_credit_history', 'data_type': 'numeric'}, {'column_name': 'feat_months_since_last_delinquent', 'data_type': 'integer'}, {'column_name': 'feat_number_of_open_accounts', 'data_type': 'integer'}, {'column_name': 'feat_number_of_credit_problems', 'data_type': 'integer'}, {'column_name': 'feat_current_credit_balance', 'data_type': 'integer'}, {'column_name': 'feat_maximum_open_credit', 'data_type': 'integer'}, {'column_name': 'feat_bankruptcies', 'data_type': 'numeric'}, {'column_name': 'id', 'data_type': 'uuid'}, {'column_name': 'timestamp', 'data_type': 'timestamp without time zone'}, {'column_name': 'model_id', 'data_type': 'integer'}, {'column_name': 'event_type', 'data_type': 'text'}, {'column_name': 'log_id', 'data_type': 'text'}, {'column_name': 'created_at', 'data_type': 'timestamp without time zone'}, {'column_name': 'updated_at', 'data_type': 'timestamp with time zone'}, {'column_name': 'prediction_score_loan_status', 'data_type': 'numeric'}, {'column_name': 'prediction_class_loan_status', 'data_type': 'numeric'}, {'column_name': 'actual_loan_status', 'data_type': 'integer'}]\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "conn = psycopg2.connect(\"host=dev-db0.caybfw3odtop.us-east-1.rds.amazonaws.com dbname=analyzedb user=postgres password=esZtEADmJ8hqx7z\")\n",
    "cur = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "\n",
    "cur.execute(\"\"\"select row_to_json(t) from (SELECT column_name,data_type FROM information_schema.columns WHERE table_name = 'dataset_loan_profile_1183') t;\"\"\")\n",
    "\n",
    "columns=[i[0] for i in cur.fetchall()]\n",
    "\n",
    "    \n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c2de9-9c93-498a-9e9e-6ae0587efcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39c9226d-e46e-411c-802b-ba89372fb492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integer\n",
      "integer\n",
      "integer\n",
      "integer\n",
      "integer\n",
      "integer\n",
      "numeric\n",
      "numeric\n",
      "numeric\n",
      "integer\n",
      "integer\n",
      "integer\n",
      "integer\n",
      "integer\n",
      "numeric\n",
      "uuid\n",
      "timestamp without time zone\n",
      "integer\n",
      "text\n",
      "text\n",
      "timestamp without time zone\n",
      "timestamp with time zone\n",
      "numeric\n",
      "numeric\n",
      "integer\n",
      "feat_loan_status Nullable(Int32) DEFAULT NULL ,feat_current_loan_amount Nullable(Int32) DEFAULT NULL ,feat_term Nullable(Int32) DEFAULT NULL ,feat_credit_score Nullable(Int32) DEFAULT NULL ,feat_years_in_current_job Nullable(Int32) DEFAULT NULL ,feat_home_ownership Nullable(Int32) DEFAULT NULL ,feat_annual_income Nullable(Float64) DEFAULT NULL ,feat_monthly_debt Nullable(Float64) DEFAULT NULL ,feat_years_of_credit_history Nullable(Float64) DEFAULT NULL ,feat_months_since_last_delinquent Nullable(Int32) DEFAULT NULL ,feat_number_of_open_accounts Nullable(Int32) DEFAULT NULL ,feat_number_of_credit_problems Nullable(Int32) DEFAULT NULL ,feat_current_credit_balance Nullable(Int32) DEFAULT NULL ,feat_maximum_open_credit Nullable(Int32) DEFAULT NULL ,feat_bankruptcies Nullable(Float64) DEFAULT NULL ,id UUID default generateUUIDv4() ,timestamp timestamp default now() ,model_id Nullable(Int32) DEFAULT NULL ,event_type Nullable(String) DEFAULT NULL ,log_id Nullable(String) DEFAULT NULL ,created_at timestamp default now() ,updated_at timestamp default now() ,prediction_score_loan_status Nullable(Float64) DEFAULT NULL ,prediction_class_loan_status Nullable(Float64) DEFAULT NULL ,actual_loan_status Nullable(Int32) DEFAULT NULL \n"
     ]
    }
   ],
   "source": [
    "    raw_col_query=\"\"\n",
    "    ch_driver_obj = get_instance()\n",
    "    for i in range(len(columns)):\n",
    "        raw_col_query+=str(columns[i][\"column_name\"]).lower().replace(' ','_')+\" \"+str(ch_driver_obj.types_mapper(columns[i][\"data_type\"]))+\" \"\n",
    "        if i != len(columns)-1:\n",
    "            raw_col_query+=\",\"\n",
    "    #raw_col_query+=\", id\" + \" \" + \"UUID default generateUUIDv4() \"\n",
    "            \n",
    "    print(raw_col_query)    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "207a6d94-a35c-4c0d-93fb-d0e85046ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dynamic_table_query(table_name,raw_col_placeholders)->str:\n",
    "    return '''create table {table_name} ({whole_str})  ENGINE = MergeTree() PRIMARY KEY id'''.format(table_name=table_name,whole_str=raw_col_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70a19949-9c0a-49c8-b1e9-2761ea9d1927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create table dataset_loan_profile_1183 (feat_loan_status Nullable(Int32) DEFAULT NULL ,feat_current_loan_amount Nullable(Int32) DEFAULT NULL ,feat_term Nullable(Int32) DEFAULT NULL ,feat_credit_score Nullable(Int32) DEFAULT NULL ,feat_years_in_current_job Nullable(Int32) DEFAULT NULL ,feat_home_ownership Nullable(Int32) DEFAULT NULL ,feat_annual_income Nullable(Float64) DEFAULT NULL ,feat_monthly_debt Nullable(Float64) DEFAULT NULL ,feat_years_of_credit_history Nullable(Float64) DEFAULT NULL ,feat_months_since_last_delinquent Nullable(Int32) DEFAULT NULL ,feat_number_of_open_accounts Nullable(Int32) DEFAULT NULL ,feat_number_of_credit_problems Nullable(Int32) DEFAULT NULL ,feat_current_credit_balance Nullable(Int32) DEFAULT NULL ,feat_maximum_open_credit Nullable(Int32) DEFAULT NULL ,feat_bankruptcies Nullable(Float64) DEFAULT NULL ,id UUID default generateUUIDv4() ,timestamp timestamp default now() ,model_id Nullable(Int32) DEFAULT NULL ,event_type Nullable(String) DEFAULT NULL ,log_id Nullable(String) DEFAULT NULL ,created_at timestamp default now() ,updated_at timestamp default now() ,prediction_score_loan_status Nullable(Float64) DEFAULT NULL ,prediction_class_loan_status Nullable(Float64) DEFAULT NULL ,actual_loan_status Nullable(Int32) DEFAULT NULL )  ENGINE = MergeTree() PRIMARY KEY id\n"
     ]
    }
   ],
   "source": [
    "query=create_dynamic_table_query(\"dataset_loan_profile_1183\",raw_col_query)\n",
    "print(query)\n",
    "    # response=postgres_driver.get_instance().execute_no_return(query)\n",
    "ch_driver_obj = get_instance()\n",
    "\n",
    "response = ch_driver_obj.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "386878a1-0901-4231-88a7-2e721478f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_231657/5866108.py:6: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('dataset_loan_profile_1183.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        feat_loan_status  feat_current_loan_amount  feat_term  \\\n",
      "0                    NaN                       NaN        2.0   \n",
      "1                    NaN                    9723.0        NaN   \n",
      "2                    NaN                    7151.0        NaN   \n",
      "3                    1.0                       NaN        NaN   \n",
      "4                    NaN                   25796.0        1.0   \n",
      "...                  ...                       ...        ...   \n",
      "169333               0.0                   35329.0        0.0   \n",
      "169334               0.0                   14046.0        0.0   \n",
      "169335               0.0                    6277.0        0.0   \n",
      "169336               0.0                   28297.0        1.0   \n",
      "169337               0.0                   10138.0        0.0   \n",
      "\n",
      "        feat_credit_score  feat_years_in_current_job  feat_home_ownership  \\\n",
      "0                     NaN                        2.0                  1.0   \n",
      "1                    86.0                        9.0                  0.0   \n",
      "2                   268.0                        4.0                  2.0   \n",
      "3                    39.0                        NaN                  NaN   \n",
      "4                     NaN                       10.0                  3.0   \n",
      "...                   ...                        ...                  ...   \n",
      "169333               47.0                        5.0                  1.0   \n",
      "169334               19.0                        5.0                  0.0   \n",
      "169335                3.0                        5.0                  1.0   \n",
      "169336               21.0                        3.0                  2.0   \n",
      "169337               41.0                        9.0                  2.0   \n",
      "\n",
      "        feat_annual_income  feat_monthly_debt  feat_years_of_credit_history  \\\n",
      "0                      NaN                NaN                         13.76   \n",
      "1               1171515.96            8689.36                         16.90   \n",
      "2               1224015.72                NaN                         64.23   \n",
      "3                      NaN                NaN                           NaN   \n",
      "4               1347501.55                NaN                         13.54   \n",
      "...                    ...                ...                           ...   \n",
      "169333           181389.00            1949.94                         18.60   \n",
      "169334            93307.00            1282.97                         24.00   \n",
      "169335               -1.00             743.66                         11.50   \n",
      "169336           131378.00            2069.20                         42.00   \n",
      "169337            52718.00             672.15                         17.60   \n",
      "\n",
      "        feat_months_since_last_delinquent  ...  \\\n",
      "0                                     NaN  ...   \n",
      "1                                     NaN  ...   \n",
      "2                                     NaN  ...   \n",
      "3                                    11.0  ...   \n",
      "4                                    59.0  ...   \n",
      "...                                   ...  ...   \n",
      "169333                                1.0  ...   \n",
      "169334                                1.0  ...   \n",
      "169335                                1.0  ...   \n",
      "169336                                2.0  ...   \n",
      "169337                               14.0  ...   \n",
      "\n",
      "                                          id               timestamp  \\\n",
      "0       3a0046f3-8939-4797-903c-d3d1a2ca0f12     2022-06-01 05:32:00   \n",
      "1       6d2c9dff-c24a-4f82-b945-a1a3b6d62c8c     2022-06-01 05:34:00   \n",
      "2       52c03e0e-dda0-485e-8ffa-755973b6be62     2022-06-01 05:36:00   \n",
      "3       27e8ca11-f514-4adc-94c1-b39cf2093574     2022-06-01 05:38:00   \n",
      "4       14dfc6f2-c7bc-4a1d-8f17-951b553020f8     2022-06-01 05:40:00   \n",
      "...                                      ...                     ...   \n",
      "169333  63f64d5b-b4c8-4644-9558-0f4d680343c5  2022-09-05 14:30:39.79   \n",
      "169334  33c06183-fff7-4295-b780-cd32091f6d46  2022-09-05 14:30:39.79   \n",
      "169335  66803b3e-5a7c-4d5b-b076-b42f3905b8f4  2022-09-05 14:30:39.79   \n",
      "169336  6549aa07-6965-4685-ae11-9fc9bbe04796  2022-09-05 14:30:39.79   \n",
      "169337  eb3e0177-a830-417c-9c5b-b0aa1e4f0c3f  2022-09-05 14:30:39.79   \n",
      "\n",
      "        model_id  event_type                            log_id  \\\n",
      "0            NaN  Production  NzkphnzYIyvzYjelMHfWjiRmwtSTUUXp   \n",
      "1            NaN  Production  ggiVUGKDRzxocPegHGUGKMUqGEsQMyco   \n",
      "2            NaN  Production  iTNuEkPqAhZPDkbjZecHzbXUDzJeWJEs   \n",
      "3            NaN  Production  cuycRDGBhcDQVbwgUoIkmfmHfuMruStj   \n",
      "4            NaN  Production  RBNLUfyWYZmLEcortaJhaxYttinXJuXK   \n",
      "...          ...         ...                               ...   \n",
      "169333       NaN    Training                               NaN   \n",
      "169334       NaN    Training                               NaN   \n",
      "169335       NaN    Training                               NaN   \n",
      "169336       NaN    Training                               NaN   \n",
      "169337       NaN    Training                               NaN   \n",
      "\n",
      "                        created_at                     updated_at  \\\n",
      "0       2022-09-05 14:33:57.818851         2022-06-01 05:32:00+00   \n",
      "1       2022-09-05 14:33:57.818851         2022-06-01 05:34:00+00   \n",
      "2       2022-09-05 14:33:57.818851         2022-06-01 05:36:00+00   \n",
      "3       2022-09-05 14:33:57.818851         2022-06-01 05:38:00+00   \n",
      "4       2022-09-05 14:33:57.818851         2022-06-01 05:40:00+00   \n",
      "...                            ...                            ...   \n",
      "169333  2022-09-05 14:30:43.868119  2022-09-05 14:30:43.868119+00   \n",
      "169334  2022-09-05 14:30:43.868119  2022-09-05 14:30:43.868119+00   \n",
      "169335  2022-09-05 14:30:43.868119  2022-09-05 14:30:43.868119+00   \n",
      "169336  2022-09-05 14:30:43.868119  2022-09-05 14:30:43.868119+00   \n",
      "169337  2022-09-05 14:30:43.868119  2022-09-05 14:30:43.868119+00   \n",
      "\n",
      "        prediction_score_loan_status prediction_class_loan_status  \\\n",
      "0                                NaN                          0.0   \n",
      "1                                NaN                          1.0   \n",
      "2                                NaN                          0.0   \n",
      "3                                NaN                          0.0   \n",
      "4                                NaN                          0.0   \n",
      "...                              ...                          ...   \n",
      "169333                           NaN                          NaN   \n",
      "169334                           NaN                          NaN   \n",
      "169335                           NaN                          NaN   \n",
      "169336                           NaN                          NaN   \n",
      "169337                           NaN                          NaN   \n",
      "\n",
      "       actual_loan_status  \n",
      "0                       0  \n",
      "1                       1  \n",
      "2                       0  \n",
      "3                       0  \n",
      "4                       0  \n",
      "...                   ...  \n",
      "169333                  0  \n",
      "169334                  0  \n",
      "169335                  0  \n",
      "169336                  0  \n",
      "169337                  0  \n",
      "\n",
      "[169338 rows x 25 columns]\n",
      "feat_loan_status                             object\n",
      "feat_current_loan_amount                     object\n",
      "feat_term                                    object\n",
      "feat_credit_score                            object\n",
      "feat_years_in_current_job                    object\n",
      "feat_home_ownership                          object\n",
      "feat_annual_income                           object\n",
      "feat_monthly_debt                            object\n",
      "feat_years_of_credit_history                 object\n",
      "feat_months_since_last_delinquent            object\n",
      "feat_number_of_open_accounts                 object\n",
      "feat_number_of_credit_problems               object\n",
      "feat_current_credit_balance                  object\n",
      "feat_maximum_open_credit                     object\n",
      "feat_bankruptcies                            object\n",
      "id                                           object\n",
      "timestamp                            datetime64[ns]\n",
      "model_id                                     object\n",
      "event_type                                   object\n",
      "log_id                                       object\n",
      "created_at                           datetime64[ns]\n",
      "updated_at                           datetime64[ns]\n",
      "prediction_score_loan_status                 object\n",
      "prediction_class_loan_status                 object\n",
      "actual_loan_status                            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "from clickhouse_driver import connect,Client\n",
    "import os\n",
    "df = pd.read_csv('dataset_loan_profile_1183.csv')\n",
    "print(df)\n",
    "data = df.values.tolist()\n",
    "string_buffer = io.StringIO()\n",
    "csv_writer = csv.writer(string_buffer)\n",
    "csv_writer.writerows(data)\n",
    "string_buffer.seek(0)\n",
    "df = df.astype({\"timestamp\": np.datetime64})\n",
    "df = df.astype({\"updated_at\": np.datetime64})\n",
    "df = df.astype({\"created_at\": np.datetime64})\n",
    "df= df.replace({np.nan: None})\n",
    "print(df.dtypes)\n",
    "df = df.convert_dtypes()\n",
    "df[\"timestamp\"] = df[\"timestamp\"].map(lambda col:np.datetime64(col))\n",
    "df[\"updated_at\"]=df[\"updated_at\"].map(lambda col:np.datetime64(col))\n",
    "df[\"created_at\"]=df[\"created_at\"].map(lambda col:np.datetime64(col))\n",
    "df= df.replace({np.nan: None})\n",
    "table_name = \"dataset_loan_profile_1183\"\n",
    "response=get_instance().insert_dataframe_dict(f'INSERT INTO {table_name} VALUES', df.to_dict('records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62b56b-661c-4e13-a9d7-206481a84cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
